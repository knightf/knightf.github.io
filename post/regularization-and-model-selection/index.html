    <!DOCTYPE html>
<html lang="zh">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="韩飞">
    
    <meta name="generator" content="Hugo 0.18.1" />
    <title>机器学习 第七章 一般化与模型选择 &middot; 韩飞的博客</title>
    <link rel="shortcut icon" href="https://knightf.github.io/images/favicon.ico">
    <link rel="stylesheet" href="https://knightf.github.io/css/style.css">
    <link rel="stylesheet" href="https://knightf.github.io/css/highlight.css">
    

    
    <link rel="stylesheet" href="https://knightf.github.io/css/monosocialiconsfont.css">
    

    

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [['$','$'], ['\\(','\\)']],
          displayMath: [['$$','$$'], ['\[','\]']],
          processEscapes: true,
          processEnvironments: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
          TeX: { equationNumbers: { autoNumber: "AMS" },
               extensions: ["AMSmath.js", "AMSsymbols.js"] }
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
          
          
          
          var all = MathJax.Hub.getAllJax(), i;
          for(i = 0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
    </script>
  </head>

    <body>
       <nav class="main-nav">
	
	
		<a href='https://knightf.github.io/'> <span class="arrow">←</span>Home</a>
	

	
		
	

	
</nav>

        <section id="wrapper">
            <article class="post">
                <header>
                    <h1>机器学习 第七章 一般化与模型选择</h1>
                    <h2 class="headline">
                    April 28, 2017 
                    <br>
                    
                    
                        
                            <a href="https://knightf.github.io/tags/machine-learning">Machine Learning</a>
                        
                            <a href="https://knightf.github.io/tags/stanford-cs229">Stanford CS229</a>
                        
                    
                    
                    </h2>
                </header>
                <section id="post-body">
                    

<p>考虑自动化解决平衡偏差与方差的问题。例如自动选择就地权重回归中的带宽的值 <code>$\tau$</code> ，又或者是支持向量机中的参数 <code>$C$</code> 。</p>

<h1 id="交叉验证">交叉验证</h1>

<p><code>简单交叉验证</code> (Simple Cross Validation) 中步骤如下：</p>

<ul>
<li>随机将训练集 <code>$S$</code> 分成 <code>$S_{train}$</code> (70%) 和 <code>$S_{CV}$</code> (30%) 。</li>
<li>对每一个 <code>$M_{i}$</code> 都用 <code>$S_{train}$</code> 训练，得到假设函数 <code>$h_{i}$</code> 。</li>
<li>选择在交叉验证的数据集上误差 <code>$\hat{\epsilon}_{S_{CV}}(h_{i})$</code> 最小的 <code>$h_{i}$</code> 。</li>
</ul>

<p>这样验证的不利之处在于浪费了 30% 的训练集。如果训练数据充分没有问题，但如果数据不充分可以用这种验证方法，叫做 <code>k 重交叉验证</code> (K-fold Cross Validation) ，每次保留的数据比例很少。</p>

<ul>
<li>把训练集 <code>$S$</code> 等数量分为 <code>$k$</code> 个互不交叉的子集 <code>$S_1, \ldots , S_k$</code> 。</li>
<li>对每个可用的模型 <code>$M_i$</code> ，分别计算出 <code>$k$</code> 个假设函数 <code>$h_{ij}$</code> 。每次计算时使用的训练集都去掉子集 <code>$S_j$</code> 。然后用子集 <code>$S_j$</code> 验证假设函数 <code>$h_{ij}$</code> 的误差 <code>$\hat{\epsilon}_{S_j}(h_{ij})$</code> 。最后计算出所有假设函数误差的平均数，衡量 <code>$M_i$</code> 的误差。</li>
<li>选出误差最小的 <code>$M_i$</code> ，并使用整个训练集 <code>$S$</code> 得到最佳的假设函数。</li>
</ul>

<p>通常 <code>$k=10$</code> 。如果数据很离散，也可以令 <code>$k=m$</code> ，这样每次只剔除一条记录。这样的验证方式叫做 <code>除一交叉验证</code> (Leave-one-out Cross Validation) 。</p>

<h1 id="特征选择">特征选择</h1>

<p>考虑 <code>$n$</code> 远远大于 <code>$m$</code> 的情况。问题在于如何从众多的特征中选出与本次学习有关的几个。如果有 <code>$n$</code> 个特征，则有 <code>$2^{n}$</code> 种可能的特征选择。下面描述的过程叫做 <code>正向搜索</code> (Forward Search) :</p>

<ul>
<li>初始 <code>$\mathcal{F} = \emptyset$</code> 。</li>
<li>重复以下步骤：对于 <code>$i=1,\ldots,n$</code> ，如果 <code>$i\notin\mathcal{F}$</code> 则令 <code>$\mathcal{F}_{i}=\mathcal{F}\cup\{i\}$</code> 。然后选择一种交叉验证的方法评价 <code>$\mathcal{F}_i$</code> 。也就是只用 <code>$\mathcal{F}_i$</code> 中的特征估计出误差。令 <code>$\mathcal{F}$</code> 等于这一步找到的最佳特征集合。</li>
<li>选出所有的特征子集中最佳的一个。</li>
</ul>

<p>循环结束的条件可以是 <code>$|\mathcal{F}|$</code> 超过了之前设置好的数量或者 <code>$\mathcal{F}$</code> 已经囊括了所有的特征。也有 <code>反向搜索</code> (Backward Search) ，与正向搜索相反，它从囊括所有特征的集合中一个个剔除直到收敛。</p>

<p><code>筛选特征选择</code> (Filter Feature Selection) 过程简单而且更好理解。这种方法试图计算出 <code>$S(i)$</code> 作为评价特征 <code>$x_{i}$</code> 与标签 <code>$y$</code> 的相关程度。然后我们选择出 <code>$k$</code> 个相关程度最高的特征。</p>

<p>实际执行的过程中通常选择 <code>$x_{i}$</code> 与 <code>$y$</code> 之间的 <code>共同信息</code> (Mutual Information) 作为 <code>$S(i)$</code> ：</p>

<p><code>$$MI(x_{i}, y)=\sum_{x_{i}\in\{0,1\}}{\sum_{y\in\{0,1\}}{p(x_{i},y)\log{\frac{p(x_{i},y)}{p(x_{i})p(y)}}}}$$</code></p>

<h1 id="贝叶斯统计与一般化">贝叶斯统计与一般化</h1>

<p>之前选择参数 <code>$\theta$</code> 用的是下面的公式：</p>

<p><code>$$\theta_{ML}=\arg\max_{\theta}\prod_{i=1}^{m}{p(y^{(i)}|x^{(i)};\theta)}$$</code></p>

<p>这样求出的值是应用 <code>频率统计</code> 学派的思想，即参数已确定只是未知。而使用 <code>贝叶斯统计</code> 学派的思想， <code>$\theta$</code> 则是一个未知的随机变量。在这种方法中，定义 <code>先验概率</code> <code>$p(\theta)$</code> 代表对于参数的先验估计。使用训练集中的记录 <code>$x$</code> 可以计算得到参数关于训练集的后验概率分布：</p>

<p><code>$$
\begin{eqnarray}
p(\theta|S) &amp;=&amp; \frac{p(S|\theta)p(\theta)}{p(S)} \\
&amp;=&amp; \frac{(\prod_{i=1}^{m}{p(y^{(i)}|x^{(i)},\theta)})p(\theta)}{\int_{\theta}{\prod_{i=1}^{m}{p(y^{(i)}|x^{(i)},\theta)p(\theta)}}d\theta}
\end{eqnarray}
$$</code></p>

<p>上面式子中的 <code>$p(y^{(i)}|x^{(i)},\theta)$</code> 通过选取的模型预测出的结果得到。而当给出测试集中的记录 <code>$x$</code> 时，可以计算标签 <code>$y$</code> 关于记录和测试集的概率分布：</p>

<p><code>$$p(y|x,S)=\int_{\theta}{p(y|x;\theta)p(\theta|S)}d\theta$$</code></p>

<p>其中 <code>$p(\theta|S)$</code> 可以通过再之前的式子得到。如果需要计算给定 <code>$x$</code> 时 <code>$y$</code> 的期望，就有：</p>

<p><code>$$E[y|x,S]=\int_{y}{yp(y|x,S)}dy$$</code></p>

<p>这样的流程是完整的贝叶斯预测过程。通常这样的后验概率很难计算，因为需要对 <code>$\theta$</code> 积分。所以一般将对 <code>$\theta$</code> 的后验概率分布替换为一个单一的估计值 <code>最大后验概率</code> (Maximum a Posteriori) ：</p>

<p><code>$$\theta_{MAP}=\arg\max_{\theta}{\prod_{i=1}^{m}{p(y^{(i)}|x^{(i)},\theta)p(\theta)}}$$</code></p>

<p>注意这个方程和最大似然估计的方程类似，只在最后添加了 <code>$p(\theta)$</code> 。实际应用过程中常用的先验概率模型使用 <code>$\theta\sim\mathcal{N}(0, \tau^{2}I)$</code> 。采用这样的概率模型得到的 <code>$\theta_{MAP}$</code> 通常范数比最大似然估计得到的 <code>$\theta$</code> 的范数更小。这使得贝叶斯 MAP 估计比起最大似然估计更不易受到过度拟合的影响。比如贝叶斯逻辑回归，虽然它的 <code>$n\gg{m}$</code>，但还是一个有效的文字分类方法。</p>

                </section>
            </article>
            

            

            <ul id="post-list" class="archive readmore">
    <h3>Read more</h3>
    
    
        
        <li>
            <a href="https://knightf.github.io/post/reinforcement-learning-and-control/">机器学习 第十三章 强化学习与控制<aside class="dates">Jul 4</aside></a>
        </li>
        
   
    
        
        <li>
            <a href="https://knightf.github.io/post/independent-components-analysis/">机器学习 第十二章 独立成分分析<aside class="dates">Jun 2</aside></a>
        </li>
        
   
    
        
        <li>
            <a href="https://knightf.github.io/post/principal-components-analysis/">机器学习 第十一章 主成分分析<aside class="dates">Jun 1</aside></a>
        </li>
        
   
    
        
        <li>
            <a href="https://knightf.github.io/post/factor-analysis/">机器学习 第十章 因子分析<aside class="dates">May 31</aside></a>
        </li>
        
   
    
        
        <li>
            <a href="https://knightf.github.io/post/the-em-algorithm/">机器学习 第九章 EM 算法<aside class="dates">May 18</aside></a>
        </li>
        
   
    
        
        <li>
            <a href="https://knightf.github.io/post/the-perceptron-and-k-means/">机器学习 第八章 在线学习与K-means算法<aside class="dates">May 12</aside></a>
        </li>
        
   
    
        
        <li>
            <a href="https://knightf.github.io/post/regularization-and-model-selection/">机器学习 第七章 一般化与模型选择<aside class="dates">Apr 28</aside></a>
        </li>
        
   
    
        
        <li>
            <a href="https://knightf.github.io/post/learning-theory/">机器学习 第六章 机器学习理论<aside class="dates">Mar 24</aside></a>
        </li>
        
   
    
        
        <li>
            <a href="https://knightf.github.io/post/support-vector-machine/">机器学习 第五章 支持向量机<aside class="dates">Mar 10</aside></a>
        </li>
        
   
    
        
        <li>
            <a href="https://knightf.github.io/post/generative-learning-algorithms/">机器学习 第四章 生成学习算法<aside class="dates">Mar 7</aside></a>
        </li>
        
   
</ul>
            <footer id="footer">
    
        <div id="social">

	
	
    
    <a class="symbol" href="mailto:knightf@live.cn">
        circleemail
    </a>
    
    <a class="symbol" href="https://github.com/knightf">
        circlegithub
    </a>
    
    <a class="symbol" href="https://twitter.com/Errrrrrrrrrric">
        circletwitterbird
    </a>
    


</div>

    
    <p class="small">
    
        © Copyright 2017 韩飞
    
    </p>
</footer>

        </section>

        <script src="//cdn.bootcss.com/jquery/2.1.1/jquery.min.js"></script>
<script src="https://knightf.github.io/js/main.js"></script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<script src="//cdn.bootcss.com/highlight.js/9.4.0/highlight.min.js"></script>
<script >hljs.initHighlightingOnLoad();</script>  


    </body>
</html>

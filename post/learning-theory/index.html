    <!DOCTYPE html>
<html lang="zh">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="韩飞">
    
    <meta name="generator" content="Hugo 0.18.1" />
    <title>机器学习 第六章 机器学习理论 &middot; 韩飞的博客</title>
    <link rel="shortcut icon" href="https://knightf.github.io/images/favicon.ico">
    <link rel="stylesheet" href="https://knightf.github.io/css/style.css">
    <link rel="stylesheet" href="https://knightf.github.io/css/highlight.css">
    

    
    <link rel="stylesheet" href="https://knightf.github.io/css/monosocialiconsfont.css">
    

    

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [['$','$'], ['\\(','\\)']],
          displayMath: [['$$','$$'], ['\[','\]']],
          processEscapes: true,
          processEnvironments: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
          TeX: { equationNumbers: { autoNumber: "AMS" },
               extensions: ["AMSmath.js", "AMSsymbols.js"] }
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
          
          
          
          var all = MathJax.Hub.getAllJax(), i;
          for(i = 0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
    </script>
  </head>

    <body>
       <nav class="main-nav">
	
	
		<a href='https://knightf.github.io/'> <span class="arrow">←</span>Home</a>
	

	
		
	

	
</nav>

        <section id="wrapper">
            <article class="post">
                <header>
                    <h1>机器学习 第六章 机器学习理论</h1>
                    <h2 class="headline">
                    March 24, 2017 
                    <br>
                    
                    
                        
                            <a href="https://knightf.github.io/tags/machine-learning">Machine Learning</a>
                        
                            <a href="https://knightf.github.io/tags/stanford-cs229">Stanford CS229</a>
                        
                    
                    
                    </h2>
                </header>
                <section id="post-body">
                    

<h1 id="误差-方差的权衡">误差/方差的权衡</h1>

<p>之前的例子中，用低次或高次多项式的曲线拟合数据会产生高误差和高方差的问题。也就是欠拟合和过拟合。所以可见误差和方差需要人工权衡，来减少之后实际应用模型时发生的误差。定义一个假设函数的 <code>一般误差</code> 为它对于所有(也就是不局限于训练集)样本的误差期望。</p>

<h1 id="一些铺垫">一些铺垫</h1>

<p>怎么才能量化表示误差/方差的此消彼长呢？为什么在训练集上误差变小就一定代表它应对其他样本时的误差也小呢？训练误差与一般误差有联系么？在什么样的条件下学习算法才能有效工作？解答这些问题有助于落实之前的理论。</p>

<p><strong>The union bound 引理</strong> <code>$A_1,A_2,\ldots,A_k$</code> 表示 <code>$k$</code> 个不同的事件(不一定相互独立)。有：</p>

<p><code>$$P(A_{1} \cup \cdots \cup A_{k}) \leq P(A_{1})+ \ldots +P(A_{k})$$</code></p>

<p><strong>Hoeffding inequality 引理</strong> 从一个伯努利分布中取出 <code>$m$</code> 个独立随机变量，用 <code>$Z_1,ldots,Z_m$</code> 表示，它们服从 iid 分布。例如， <code>$P(Z_{i}=1)=\phi$</code> ，<code>$P(Z_{i}=0)=1-\phi$</code> 。这些随机变量的均值为 <code>$\hat{\phi}=(1/m)\sum_{i=1}^{m}Z_{i}$</code> ，有 <code>$\gamma&gt;0$</code> 固定，则：</p>

<p><code>$$P(|\phi-\hat{\phi}|&gt;\gamma)\leq2\exp{(-2\gamma^{2}m)}$$</code></p>

<p>只用这两个引理，就可以证明学习理论中一些最深奥也最重要的结果。为了简化过程，考虑二元分类器 <code>$y\in{0,1}$</code> 。有大小为 <code>$m$</code> 的训练集 <code>$S=\{(x^{(i)},y^{(i)});i=1,\ldots,m\}$</code> ，训练样本均来自 <code>$\mathcal{D}$</code> 符合 iid 。对假设 <code>$h$</code> ，它的训练误差为：</p>

<p><code>$$\hat{\epsilon}(h)=\frac{1}{m}\sum_{i=1}^{m}1\{h(x^{(i)}\neq y^{(i)})\}$$</code></p>

<p>上式表示假设 <code>$h$</code> 对训练样本判断错误的比例。定义一般误差为：</p>

<p><code>$$\epsilon(h)=P_{(x,y)\sim\mathcal{D}}(h(x) \neq y)$$</code></p>

<p>它和 <code>$\hat{\epsilon}$</code> 很类似，只不过它的样本不一定来自训练集。考虑线性分类问题， <code>$h_{\theta}(x)=1\{\theta^{T}x \geq 0\}$</code> 。找到最合适的参数 <code>$\theta$</code> ，一个方法是选择训练误差最小时的 <code>$\hat{\theta}$</code> ：</p>

<p><code>$$\hat{\theta}=\arg{\min_{\theta}\hat{\epsilon}(h_{\theta})}$$</code></p>

<p>这一过程称作 <code>经验主义风险最小化</code> (empirical risk minimization) ，由它得到的假设函数 <code>$\hat{h}=h_{\hat{\theta}}$</code> 。定义 <code>假设集合</code> (hypothesis class) <code>$\mathcal{H}$</code> 是囊括了一种学习算法下所有可能的分类器的集合。对于线性分类就有 <code>$\mathcal{H}=\{h_{\theta}:h_{\theta}(x)=1\{\theta^{T}x\geq{0}\},\theta\in\mathbb{R}^{n+1}\}$</code> 。从更广泛的意义上说， <code>$\mathcal{H}$</code> 也可以表示某种具有特定结构的神经网络。</p>

<p>经验主义风险最小化现在就可以被表示成一个从某一类学习算法的 <code>$\mathcal{H}$</code> 中挑选最佳假设的过程：</p>

<p><code>$$\hat{h}=\arg{\min_{h\in\mathcal{H}}\hat{\epsilon}(h)}$$</code></p>

<h1 id="mathcal-h-是有限集合时"><code>$\mathcal{H}$</code> 是有限集合时</h1>

<p>考虑 <code>$\mathcal{H}$</code> 中有 <code>$k$</code> 个假设时的情况。首先证明对于所有 <code>$h$</code> ， <code>$\hat{\epsilon}(h)$</code> 是一个对于 <code>$\epsilon(h)$</code> 可信的估计。从 <code>$\mathcal{H}$</code> 中选出某 <code>$h_{i}$</code> 。定义一个伯努利随机变量 <code>$Z$</code> ，它的分布按照如下定义。从 <code>$\mathcal{D}$</code> 中抽样选出 <code>$(x,y)$</code> ，令 <code>$Z=1\{h_{i}(x)\neq{y}\}$</code> 。同样的，定义 <code>$Z_{j}=1\{h_{i}(x^{(j)}\neq{y^{(j)}})\}$</code> 。由于训练样本都是从 <code>$\mathcal{D}$</code> 中以 iid 方式选出，所以 <code>$Z$</code> 和 <code>$Z_{j}$</code> 具有有相同的分布。</p>

<p>所以对一个随机选取的样本错误分类的概率 <code>$\epsilon(h)$</code> 就是 <code>$Z$</code> 和 <code>$Z_{j}$</code> 的期望：</p>

<p><code>$$\hat{\epsilon}(h_{i})=\frac{1}{m}\sum_{j=1}^{m}Z_{j}$$</code></p>

<p>应用 Hoeffiding inequality 引理有：</p>

<p><code>$$P(|\epsilon(h_{i})-\hat{\epsilon}(h_{i})|&gt;\gamma)\leq2\exp{(-2\gamma^{2}m)}$$</code></p>

<p>说明在 <code>$m$</code> 的值很大的情况下对于一个给定的 <code>$h_{i}$</code> ，训练误差和一般误差有很大的概率会十分接近。为了证明对于所有 <code>$h \in \mathcal{H}$</code> 都有这个规律，令 <code>$A_{i}$</code> 表示事件 <code>$|\epsilon(h_{i})-\hat{\epsilon}(h_{i})|&gt;\gamma$</code> 。所以对于任意 <code>$A_{i}$</code> ，都有 <code>$P(A_{i})\leq{2\exp{(-2\gamma^{2}m)}}$</code> 。应用 The union bound 引理：</p>

<p><code>$$
\begin{eqnarray}
P(\exists{h\in\mathcal{H}}.|\epsilon(h_{i})-\hat{\epsilon}(h_{i})&gt;\gamma|) &amp;=&amp; P(A_{1}\cup\cdots\cup{A_{k}}) \\
&amp;\leq&amp; \sum_{i=1}^{k}P(A_{i}) \\
&amp;\leq&amp; \sum_{i=1}^{k}2\exp{(-2\gamma^{2}m)} \\
&amp;=&amp; 2k\exp{(-2\gamma^{2}m)}
\end{eqnarray}
$$</code></p>

<p>同时取非，得到对于所有 <code>$h\in\mathcal{H}$</code> ， <code>$\hat{\epsilon}(h)$</code> 和 <code>$\epsilon(h)$</code> 的差都小于等于 <code>$\gamma$</code> 。设 <code>$\delta=2k\exp{(-2\gamma^{2}m)}$</code> 则有：</p>

<p><code>$$m \geq \frac{1}{2\gamma^{2}}\log{\frac{2k}{\delta}}$$</code></p>

<p>上式如果满足，则 <code>$|\epsilon(h)-\hat{\epsilon(h)}|\leq \gamma$</code> 对 <code>$h\in\mathcal{H}$</code> 成立的概率至少为 <code>$1-\delta$</code> 。一个给定的算法达到一定水平的表现所需训练集的大小 <code>$m$</code> 叫做该算法的 <code>样本复杂度</code> (sample complexity) 。</p>

<p>定义 <code>$h^{*}=\arg{\min_{h\in\mathcal{H}}{\epsilon(h)}}$</code> 为 <code>$\mathcal{H}$</code> 中最佳的假设。注意这里 <code>$h^{*}$</code> 和 <code>$\hat{h}$</code> 的区别。有以下关系：</p>

<p><code>$$
\begin{eqnarray}
\epsilon(\hat{h}) &amp;\leq&amp; \hat{\epsilon}(\hat{h})+\gamma \\
&amp;\leq&amp; \hat{\epsilon}(h^{*})+\gamma \\
&amp;\leq&amp; \epsilon(h^{*})+2\gamma
\end{eqnarray}
$$</code></p>

<p>上面的关系表明， <code>$\hat{h}$</code> 的一般误差最多比 <code>$\mathcal{H}$</code> 中最佳的假设差出 <code>$2\gamma$</code> 。可以给出定理：</p>

<p><strong>定理</strong> 令 <code>$|\mathcal{H}|=k$</code> ，并固定 <code>$m, \delta$</code> 的值。至少有 <code>$1-\delta$</code> 的概率有：</p>

<p><code>$$\epsilon(\hat{h})\leq(\min_{h\in\mathcal{H}}\epsilon(h))+2\sqrt{\frac{1}{2m}\log{\frac{2k}{\delta}}}$$</code></p>

<p><strong>推论</strong> 固定 <code>$\gamma$</code> 和 <code>$\delta$</code> 的值，并且对 <code>$m$</code> 求解，我们可以得到这些限制：</p>

<p><code>$$
\begin{eqnarray}
m &amp;\geq&amp; \frac{1}{2\gamma^2} \log{\frac{2k}{\delta}} \\ 
&amp;=&amp; O(\frac{1}{\gamma^2} \log{\frac{k}{\delta}})
\end{eqnarray}
$$</code></p>

<h1 id="mathcal-h-是无限集合时"><code>$\mathcal{H}$</code> 是无限集合时</h1>

<p>给定一个点的集合 <code>$S=\{x^{(i)},\ldots,x^{(d)}\}$</code> ，如果 <code>$\mathcal{H}$</code> 可以把集合 <code>$S$</code> 中任意组合的点区分开，那么它就 <code>打碎</code> 了这个集合。给定某个假设的集合 <code>$\mathcal{H}$</code> ，定义它可以打碎的最大的集合的数量为它的 <code>VC纬度</code> ，写作 <code>$VC(\mathcal{H})$</code> 。</p>

<p>例如，平面上有 3 个点，无论它们的标签如何分配，如何组合，包含形如 <code>$(h(x)=1\{\theta_0+\theta_1 x_1+\theta_2 x_2 \geq 0\})$</code> 假设的假设集合 <code>$\mathcal{H}$</code> 都以把它们分开，所以 <code>$VC(\mathcal{H})=3$</code> 。</p>

<p><strong>定理</strong> 给定 <code>$\mathcal{H}$</code> ，令 <code>$d=VC(\mathcal{H})$</code> 。则有至少为 <code>$1-\delta$</code> 的概率，对于所有 <code>$h\in\mathcal{H}$</code> 有：</p>

<p><code>$$|\epsilon(h)-\hat{\epsilon}(h)| \leq O(\sqrt{\frac{d}{m}\log{\frac{m}{d}}+\frac{1}{m}\log{\frac{1}{\delta}}})$$</code></p>

<p>换句话说，如果一个假设集有有限的 VC 维度，那么 <code>$m$</code> 很大时就会发生统一收敛。同样有以下推论：</p>

<p><strong>推论</strong> 令 <code>$|\epsilon(h)-\hat{\epsilon}(h)| \leq \gamma$</code> 至少以 <code>$1-\delta$</code> 的概率对所有 <code>$h\in\mathcal{H}$</code> 成立 (也就是 <code>$\epsilon(\hat{h})\leq\epsilon(h^{*})+2\gamma$</code>)，它满足 <code>$m=O_{\gamma,\delta}(d)$</code> 。</p>

<p>可以得到的结论是必要的训练集的大小和 <code>$\mathcal{H}$</code> 参数的数量线性相关。</p>

                </section>
            </article>
            

            

            <ul id="post-list" class="archive readmore">
    <h3>Read more</h3>
    
    
        
        <li>
            <a href="https://knightf.github.io/post/reinforcement-learning-and-control/">机器学习 第十三章 强化学习与控制<aside class="dates">Jul 4</aside></a>
        </li>
        
   
    
        
        <li>
            <a href="https://knightf.github.io/post/independent-components-analysis/">机器学习 第十二章 独立成分分析<aside class="dates">Jun 2</aside></a>
        </li>
        
   
    
        
        <li>
            <a href="https://knightf.github.io/post/principal-components-analysis/">机器学习 第十一章 主成分分析<aside class="dates">Jun 1</aside></a>
        </li>
        
   
    
        
        <li>
            <a href="https://knightf.github.io/post/factor-analysis/">机器学习 第十章 因子分析<aside class="dates">May 31</aside></a>
        </li>
        
   
    
        
        <li>
            <a href="https://knightf.github.io/post/the-em-algorithm/">机器学习 第九章 EM 算法<aside class="dates">May 18</aside></a>
        </li>
        
   
    
        
        <li>
            <a href="https://knightf.github.io/post/the-perceptron-and-k-means/">机器学习 第八章 在线学习与K-means算法<aside class="dates">May 12</aside></a>
        </li>
        
   
    
        
        <li>
            <a href="https://knightf.github.io/post/regularization-and-model-selection/">机器学习 第七章 一般化与模型选择<aside class="dates">Apr 28</aside></a>
        </li>
        
   
    
        
        <li>
            <a href="https://knightf.github.io/post/learning-theory/">机器学习 第六章 机器学习理论<aside class="dates">Mar 24</aside></a>
        </li>
        
   
    
        
        <li>
            <a href="https://knightf.github.io/post/support-vector-machine/">机器学习 第五章 支持向量机<aside class="dates">Mar 10</aside></a>
        </li>
        
   
    
        
        <li>
            <a href="https://knightf.github.io/post/generative-learning-algorithms/">机器学习 第四章 生成学习算法<aside class="dates">Mar 7</aside></a>
        </li>
        
   
</ul>
            <footer id="footer">
    
        <div id="social">

	
	
    
    <a class="symbol" href="mailto:knightf@live.cn">
        circleemail
    </a>
    
    <a class="symbol" href="https://github.com/knightf">
        circlegithub
    </a>
    
    <a class="symbol" href="https://twitter.com/Errrrrrrrrrric">
        circletwitterbird
    </a>
    


</div>

    
    <p class="small">
    
        © Copyright 2017 韩飞
    
    </p>
</footer>

        </section>

        <script src="//cdn.bootcss.com/jquery/2.1.1/jquery.min.js"></script>
<script src="https://knightf.github.io/js/main.js"></script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<script src="//cdn.bootcss.com/highlight.js/9.4.0/highlight.min.js"></script>
<script >hljs.initHighlightingOnLoad();</script>  


    </body>
</html>
